{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import pathlib\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "np.set_printoptions(precision=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create an input pipeline, you must start with a data source. For example, to construct a Dataset from data in memory, you can use tf.data.Dataset.from_tensors() or tf.data.Dataset.from_tensor_slices(). Alternatively, if your input data is stored in a file in the recommended TFRecord format, you can use tf.data.TFRecordDataset()."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    | Note:\n",
    "from_tensors produces a dataset containing only a single element. To slice the input tensor into multiple elements, use from_tensor_slices instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TensorSliceDataset element_spec=TensorSpec(shape=(), dtype=tf.int32, name=None)>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=[8, 3, 0, 8, 2, 1]\n",
    "dataset = tf.data.Dataset.from_tensor_slices(data)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "3\n",
      "0\n",
      "8\n",
      "2\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "for elem in dataset:\n",
    "  print(elem.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "it = iter(dataset)\n",
    "\n",
    "print(next(it).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    | The reduce transformation to compute the sum of a dataset of integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n"
     ]
    }
   ],
   "source": [
    "print(dataset.reduce(0, lambda state, value: state + value).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    | Dataset.element_spec\n",
    " The Dataset.element_spec property allows you to inspect the type of each element component. The property returns a nested structure of tf.TypeSpec objects, matching the structure of the element, which may be a single component, a tuple of components, or a nested tuple of components. For example:   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorSpec(shape=(56, 70), dtype=tf.float32, name=None)\n"
     ]
    }
   ],
   "source": [
    "dataset1 = tf.data.Dataset.from_tensor_slices(tf.random.uniform([40,56,70]))\n",
    "\n",
    "print(dataset1.element_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorSpec(shape=(), dtype=tf.float32, name=None),\n",
       " TensorSpec(shape=(100,), dtype=tf.int32, name=None))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset2 = tf.data.Dataset.from_tensor_slices(\n",
    "   (tf.random.uniform([4]),\n",
    "    tf.random.uniform([4, 100], maxval=10, dtype=tf.int32)))\n",
    "\n",
    "dataset2.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorSpec(shape=(56, 70), dtype=tf.float32, name=None),\n",
       " (TensorSpec(shape=(), dtype=tf.float32, name=None),\n",
       "  TensorSpec(shape=(100,), dtype=tf.int32, name=None)))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset3 = tf.data.Dataset.zip((dataset1, dataset2))\n",
    "\n",
    "dataset3.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseTensorSpec(TensorShape([3, 4]), tf.int32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataset containing a sparse tensor.\n",
    "dataset4 = tf.data.Dataset.from_tensors(tf.SparseTensor(indices=[[0, 0], [1, 2]], values=[1, 2], dense_shape=[3, 4]))\n",
    "\n",
    "dataset4.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.framework.sparse_tensor.SparseTensor"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use value_type to see the type of value represented by the element spec\n",
    "dataset4.element_spec.value_type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    | Note: \n",
    "The Dataset transformations support datasets of any structure. When using the Dataset.map, and Dataset.filter transformations, which apply a function to each element, the element structure determines the arguments of the function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TensorSliceDataset element_spec=TensorSpec(shape=(15,), dtype=tf.int32, name=None)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset1 = tf.data.Dataset.from_tensor_slices(\n",
    "    tf.random.uniform([4, 15], minval=0, maxval=10, dtype=tf.int32))\n",
    "\n",
    "dataset1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9 6 1 0 2 8 1 9 7 5 7 0 7 2 0]\n",
      "[4 5 4 8 3 8 0 2 3 8 8 8 3 3 9]\n",
      "[8 4 1 4 9 9 5 4 0 8 2 6 1 1 7]\n",
      "[7 4 6 6 7 8 3 7 2 4 6 4 9 6 1]\n"
     ]
    }
   ],
   "source": [
    "for z in dataset1:\n",
    "  print(z.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TensorSliceDataset element_spec=(TensorSpec(shape=(), dtype=tf.float32, name=None), TensorSpec(shape=(10,), dtype=tf.int32, name=None))>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset2 = tf.data.Dataset.from_tensor_slices(\n",
    "   (tf.random.uniform([5]),\n",
    "    tf.random.uniform([5, 10], maxval=100, dtype=tf.int32)))\n",
    "\n",
    "dataset2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ZipDataset element_spec=(TensorSpec(shape=(15,), dtype=tf.int32, name=None), (TensorSpec(shape=(), dtype=tf.float32, name=None), TensorSpec(shape=(10,), dtype=tf.int32, name=None)))>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset3 = tf.data.Dataset.zip((dataset1, dataset2))\n",
    "\n",
    "dataset3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shapes: (15,), (), (10,)\n",
      "shapes: (15,), (), (10,)\n",
      "shapes: (15,), (), (10,)\n",
      "shapes: (15,), (), (10,)\n"
     ]
    }
   ],
   "source": [
    "for a, (b,c) in dataset3:\n",
    "  print('shapes: {a.shape}, {b.shape}, {c.shape}'.format(a=a, b=b, c=c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = tf.keras.datasets.fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((array([[[0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          ...,\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0]],\n",
       "  \n",
       "         [[0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          ...,\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0]],\n",
       "  \n",
       "         [[0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          ...,\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0]],\n",
       "  \n",
       "         ...,\n",
       "  \n",
       "         [[0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          ...,\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0]],\n",
       "  \n",
       "         [[0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          ...,\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0]],\n",
       "  \n",
       "         [[0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          ...,\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0]]], dtype=uint8),\n",
       "  array([9, 0, 0, ..., 3, 0, 5], dtype=uint8)),\n",
       " (array([[[0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          ...,\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0]],\n",
       "  \n",
       "         [[0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          ...,\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0]],\n",
       "  \n",
       "         [[0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          ...,\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0]],\n",
       "  \n",
       "         ...,\n",
       "  \n",
       "         [[0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          ...,\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0]],\n",
       "  \n",
       "         [[0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          ...,\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0]],\n",
       "  \n",
       "         [[0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          ...,\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0]]], dtype=uint8),\n",
       "  array([9, 2, 1, ..., 8, 1, 5], dtype=uint8)))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = train\n",
    "images = images/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([9, 0, 0, ..., 3, 0, 5], dtype=uint8),\n",
       " array([[[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       " \n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       " \n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       " \n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       " \n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]]]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels, images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TensorSliceDataset element_spec=(TensorSpec(shape=(28, 28), dtype=tf.float64, name=None), TensorSpec(shape=(), dtype=tf.uint8, name=None))>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((images, labels))\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count(stop):\n",
    "  i = 0\n",
    "  while i<stop:\n",
    "    yield i\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object count at 0x000002260AE26490>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "for n in count(5):\n",
    "  print(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    | Consuming CSV data\n",
    "Refer to the Loading CSV Files and Loading Pandas DataFrames tutorials for more examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_file = tf.keras.utils.get_file(\"train.csv\", \"https://storage.googleapis.com/tf-datasets/titanic/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>survived</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>n_siblings_spouses</th>\n",
       "      <th>parch</th>\n",
       "      <th>fare</th>\n",
       "      <th>class</th>\n",
       "      <th>deck</th>\n",
       "      <th>embark_town</th>\n",
       "      <th>alone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>Third</td>\n",
       "      <td>unknown</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>First</td>\n",
       "      <td>C</td>\n",
       "      <td>Cherbourg</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>Third</td>\n",
       "      <td>unknown</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>First</td>\n",
       "      <td>C</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>male</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.4583</td>\n",
       "      <td>Third</td>\n",
       "      <td>unknown</td>\n",
       "      <td>Queenstown</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   survived     sex   age  n_siblings_spouses  parch     fare  class     deck  \\\n",
       "0         0    male  22.0                   1      0   7.2500  Third  unknown   \n",
       "1         1  female  38.0                   1      0  71.2833  First        C   \n",
       "2         1  female  26.0                   0      0   7.9250  Third  unknown   \n",
       "3         1  female  35.0                   1      0  53.1000  First        C   \n",
       "4         0    male  28.0                   0      0   8.4583  Third  unknown   \n",
       "\n",
       "   embark_town alone  \n",
       "0  Southampton     n  \n",
       "1    Cherbourg     n  \n",
       "2  Southampton     y  \n",
       "3  Southampton     n  \n",
       "4   Queenstown     y  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(titanic_file)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(627, 10)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['survived', 'sex', 'age', 'n_siblings_spouses', 'parch', 'fare',\n",
       "       'class', 'deck', 'embark_town', 'alone'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>survived</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>n_siblings_spouses</th>\n",
       "      <th>parch</th>\n",
       "      <th>fare</th>\n",
       "      <th>class</th>\n",
       "      <th>deck</th>\n",
       "      <th>embark_town</th>\n",
       "      <th>alone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>0</td>\n",
       "      <td>male</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9.500</td>\n",
       "      <td>Third</td>\n",
       "      <td>unknown</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>0</td>\n",
       "      <td>male</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>227.525</td>\n",
       "      <td>First</td>\n",
       "      <td>unknown</td>\n",
       "      <td>Cherbourg</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>0</td>\n",
       "      <td>male</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.650</td>\n",
       "      <td>Third</td>\n",
       "      <td>F</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>0</td>\n",
       "      <td>male</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13.000</td>\n",
       "      <td>Second</td>\n",
       "      <td>unknown</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>0</td>\n",
       "      <td>male</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>Third</td>\n",
       "      <td>unknown</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     survived   sex   age  n_siblings_spouses  parch     fare   class  \\\n",
       "320         0  male  20.0                   0      0    9.500   Third   \n",
       "400         0  male  28.0                   0      0  227.525   First   \n",
       "58          0  male  25.0                   0      0    7.650   Third   \n",
       "130         0  male  30.0                   0      0   13.000  Second   \n",
       "217         0  male  19.0                   0      0    0.000   Third   \n",
       "\n",
       "        deck  embark_town alone  \n",
       "320  unknown  Southampton     y  \n",
       "400  unknown    Cherbourg     y  \n",
       "58         F  Southampton     y  \n",
       "130  unknown  Southampton     y  \n",
       "217  unknown  Southampton     y  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TensorSliceDataset element_spec={'survived': TensorSpec(shape=(), dtype=tf.int64, name=None), 'sex': TensorSpec(shape=(), dtype=tf.string, name=None), 'age': TensorSpec(shape=(), dtype=tf.float64, name=None), 'n_siblings_spouses': TensorSpec(shape=(), dtype=tf.int64, name=None), 'parch': TensorSpec(shape=(), dtype=tf.int64, name=None), 'fare': TensorSpec(shape=(), dtype=tf.float64, name=None), 'class': TensorSpec(shape=(), dtype=tf.string, name=None), 'deck': TensorSpec(shape=(), dtype=tf.string, name=None), 'embark_town': TensorSpec(shape=(), dtype=tf.string, name=None), 'alone': TensorSpec(shape=(), dtype=tf.string, name=None)}>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic_slices = tf.data.Dataset.from_tensor_slices(dict(df))\n",
    "titanic_slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   'survived'           : 0\n",
      "   'sex'                : b'male'\n",
      "   'age'                : 22.0\n",
      "   'n_siblings_spouses' : 1\n",
      "   'parch'              : 0\n",
      "   'fare'               : 7.25\n",
      "   'class'              : b'Third'\n",
      "   'deck'               : b'unknown'\n",
      "   'embark_town'        : b'Southampton'\n",
      "   'alone'              : b'n'\n",
      "\n",
      "\n",
      "   'survived'           : 1\n",
      "   'sex'                : b'female'\n",
      "   'age'                : 38.0\n",
      "   'n_siblings_spouses' : 1\n",
      "   'parch'              : 0\n",
      "   'fare'               : 71.2833\n",
      "   'class'              : b'First'\n",
      "   'deck'               : b'C'\n",
      "   'embark_town'        : b'Cherbourg'\n",
      "   'alone'              : b'n'\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "titanic_slices = tf.data.Dataset.from_tensor_slices(dict(df))\n",
    "\n",
    "for feature_batch in titanic_slices.take(2):\n",
    "  for key, value in feature_batch.items():\n",
    "    print(\"   {!r:20s} : {}\".format(key, value)) # {!r:20s} --> specifier\n",
    "  print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    | Note:\n",
    "The tf.data.experimental.make_csv_dataset function is the high-level interface for reading sets of CSV files. It supports column type inference and many other features, like batching and shuffling, to make usage simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_batches = tf.data.experimental.make_csv_dataset(\n",
    "\n",
    "                                                        titanic_file, \n",
    "                                                        batch_size=4,\n",
    "                                                        label_name=\"survived\"\n",
    "                                                         \n",
    "                                                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset element_spec=(OrderedDict([('sex', TensorSpec(shape=(4,), dtype=tf.string, name=None)), ('age', TensorSpec(shape=(4,), dtype=tf.float32, name=None)), ('n_siblings_spouses', TensorSpec(shape=(4,), dtype=tf.int32, name=None)), ('parch', TensorSpec(shape=(4,), dtype=tf.int32, name=None)), ('fare', TensorSpec(shape=(4,), dtype=tf.float32, name=None)), ('class', TensorSpec(shape=(4,), dtype=tf.string, name=None)), ('deck', TensorSpec(shape=(4,), dtype=tf.string, name=None)), ('embark_town', TensorSpec(shape=(4,), dtype=tf.string, name=None)), ('alone', TensorSpec(shape=(4,), dtype=tf.string, name=None))]), TensorSpec(shape=(4,), dtype=tf.int32, name=None))>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'survived': [0 1 0 1]\n",
      "features:\n",
      "  'sex'               : [b'female' b'female' b'male' b'female']\n",
      "  'age'               : [28. 33. 28. 49.]\n",
      "  'n_siblings_spouses': [0 1 0 0]\n",
      "  'parch'             : [0 2 0 0]\n",
      "  'fare'              : [ 7.8958 27.75    8.05   25.9292]\n",
      "  'class'             : [b'Third' b'Second' b'Third' b'First']\n",
      "  'deck'              : [b'unknown' b'unknown' b'unknown' b'D']\n",
      "  'embark_town'       : [b'Southampton' b'Southampton' b'Southampton' b'Southampton']\n",
      "  'alone'             : [b'y' b'n' b'y' b'y']\n",
      "\n",
      "\n",
      "'survived': [0 1 1 1]\n",
      "features:\n",
      "  'sex'               : [b'female' b'male' b'female' b'female']\n",
      "  'age'               : [38. 49. 40. 40.]\n",
      "  'n_siblings_spouses': [0 1 1 0]\n",
      "  'parch'             : [0 0 1 0]\n",
      "  'fare'              : [ 13.      89.1042 134.5    153.4625]\n",
      "  'class'             : [b'Second' b'First' b'First' b'First']\n",
      "  'deck'              : [b'unknown' b'C' b'E' b'C']\n",
      "  'embark_town'       : [b'Southampton' b'Cherbourg' b'Cherbourg' b'Southampton']\n",
      "  'alone'             : [b'y' b'n' b'n' b'y']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for feature_batch, label_batch in titanic_batches.take(2):\n",
    "  print(\"'survived': {}\".format(label_batch))\n",
    "  print(\"features:\")\n",
    "  for key, value in feature_batch.items():\n",
    "    print(\"  {!r:20s}: {}\".format(key, value))\n",
    "  print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    | We can use the select_columns argument if you only need a subset of columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['survived', 'sex', 'age', 'n_siblings_spouses', 'parch', 'fare',\n",
       "       'class', 'deck', 'embark_town', 'alone'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_batches = tf.data.experimental.make_csv_dataset(\n",
    "    titanic_file, batch_size=4,\n",
    "    label_name=\"survived\", select_columns=['sex','class', 'fare', 'survived','age'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'survived': [1 0 0 1]\n",
      "  'sex'               : [b'female' b'male' b'male' b'female']\n",
      "  'age'               : [23. 23. 19. 28.]\n",
      "  'fare'              : [263.      15.0458   7.65     7.75  ]\n",
      "  'class'             : [b'First' b'Second' b'Third' b'Third']\n"
     ]
    }
   ],
   "source": [
    "for feature_batch, label_batch in titanic_batches.take(1):\n",
    "  print(\"'survived': {}\".format(label_batch))\n",
    "  for key, value in feature_batch.items():\n",
    "    print(\"  {!r:20s}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    |Note:\n",
    "There is also a lower-level experimental.CsvDataset class which provides finer grained control. It does not support column type inference. Instead you must specify the type of each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, b'male', 22.0, 1, 0, 7.25, b'Third', b'unknown', b'Southampton', b'n']\n",
      "[1, b'female', 38.0, 1, 0, 71.2833, b'First', b'C', b'Cherbourg', b'n']\n",
      "[1, b'female', 26.0, 0, 0, 7.925, b'Third', b'unknown', b'Southampton', b'y']\n",
      "[1, b'female', 35.0, 1, 0, 53.1, b'First', b'C', b'Southampton', b'n']\n",
      "[0, b'male', 28.0, 0, 0, 8.4583, b'Third', b'unknown', b'Queenstown', b'y']\n"
     ]
    }
   ],
   "source": [
    "titanic_types  = [tf.int32, tf.string, tf.float32, tf.int32, tf.int32, tf.float32, tf.string, tf.string, tf.string, tf.string]\n",
    "dataset = tf.data.experimental.CsvDataset(titanic_file, titanic_types , header=True)\n",
    "\n",
    "for line in dataset.take(5):\n",
    "  print([item.numpy() for item in line])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "flowers_root = tf.keras.utils.get_file(\n",
    "    'flower_photos',\n",
    "    'https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz',\n",
    "    untar=True\n",
    "    \n",
    "    )\n",
    "flowers_root = pathlib.Path(flowers_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('C:/Users/Laptop Point/.keras/datasets/flower_photos')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flowers_root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "daisy\n",
      "dandelion\n",
      "LICENSE.txt\n",
      "roses\n",
      "sunflowers\n",
      "tulips\n"
     ]
    }
   ],
   "source": [
    "for item in flowers_root.glob(\"*\"): # Iterate over this subtree and yield all existing files (of any kind, including directories) matching the given relative pattern.\n",
    " \n",
    "  print(item.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'C:\\\\Users\\\\Laptop Point\\\\.keras\\\\datasets\\\\flower_photos\\\\sunflowers\\\\18828277053_1493158b28.jpg'\n",
      "b'C:\\\\Users\\\\Laptop Point\\\\.keras\\\\datasets\\\\flower_photos\\\\tulips\\\\8689672277_b289909f97_n.jpg'\n",
      "b'C:\\\\Users\\\\Laptop Point\\\\.keras\\\\datasets\\\\flower_photos\\\\tulips\\\\490541142_c37e2b4191_n.jpg'\n",
      "b'C:\\\\Users\\\\Laptop Point\\\\.keras\\\\datasets\\\\flower_photos\\\\dandelion\\\\5109496141_8dcf673d43_n.jpg'\n",
      "b'C:\\\\Users\\\\Laptop Point\\\\.keras\\\\datasets\\\\flower_photos\\\\dandelion\\\\16242239484_51286673af.jpg'\n"
     ]
    }
   ],
   "source": [
    "list_ds = tf.data.Dataset.list_files(str(flowers_root/'*/*'))  # Note: The default behavior of this method is to return filenames in a non-deterministic random shuffled order. Pass a seed or shuffle=False to get results in a deterministic order.\n",
    "\n",
    "for f in list_ds.take(5):\n",
    "  print(f.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    | Read the data using the tf.io.read_file function and extract the label from the path, returning (image, label) pairs:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_path(file_path):\n",
    "  label = tf.strings.split(file_path, os.sep)[-2]\n",
    "  return tf.io.read_file(file_path), label\n",
    "\n",
    "labeled_ds = list_ds.map(process_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x01\\x00\\xf0\\x00\\xf0\\x00\\x00\\xff\\xe2\\x0cXICC_PROFILE\\x00\\x01\\x01\\x00\\x00\\x0cHLino\\x02\\x10\\x00\\x00mntrRGB XYZ \\x07\\xce\\x00\\x02\\x00\\t\\x00\\x06\\x001\\x00\\x00acspMSFT\\x00\\x00\\x00\\x00IEC sRGB\\x00\\x00\\x00\\x00\\x00\\x00'\n",
      "\n",
      "b'daisy'\n"
     ]
    }
   ],
   "source": [
    "for image_raw, label_text in labeled_ds.take(1):\n",
    "  print(repr(image_raw.numpy()[:100]))\n",
    "  print()\n",
    "  print(label_text.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    | Batching dataset elements\n",
    "\n",
    "While tf.data tries to propagate shape information, the default settings of Dataset.batch result in an unknown batch size because the last batch may not be full. Note the Nones in the shape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([0, 1, 2, 3], dtype=int64), array([ 0, -1, -2, -3], dtype=int64)]\n",
      "[array([4, 5, 6, 7], dtype=int64), array([-4, -5, -6, -7], dtype=int64)]\n"
     ]
    }
   ],
   "source": [
    "inc_dataset = tf.data.Dataset.range(10)\n",
    "dec_dataset = tf.data.Dataset.range(0, -10, -1)\n",
    "\n",
    "dataset = tf.data.Dataset.zip((inc_dataset, dec_dataset))\n",
    "batched_dataset = dataset.batch(4,drop_remainder=True)\n",
    "\n",
    "for batch in batched_dataset.take(4):\n",
    "  print([arr.numpy() for arr in batch])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    |Note:\n",
    "Use the drop_remainder argument to ignore that last batch, and get full shape propagation:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    | Batching tensors with padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0]\n",
      " [1 0 0]\n",
      " [2 2 0]\n",
      " [3 3 3]]\n",
      "\n",
      "[[4 4 4 4 0 0 0]\n",
      " [5 5 5 5 5 0 0]\n",
      " [6 6 6 6 6 6 0]\n",
      " [7 7 7 7 7 7 7]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.range(100)\n",
    "dataset = dataset.map(lambda x: tf.fill([tf.cast(x, tf.int32)], x))\n",
    "dataset = dataset.padded_batch(4, padded_shapes=(None,))\n",
    "\n",
    "for batch in dataset.take(2):\n",
    "  print(batch.numpy())\n",
    "  print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    | Training workflows\n",
    "\n",
    "Processing multiple epochs\n",
    "\n",
    "The tf.data API offers two main ways to process multiple epochs of the same data.\n",
    "\n",
    "The simplest way to iterate over a dataset in multiple epochs is to use the Dataset.repeat() transformation. First, create a dataset of titanic data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_file = tf.keras.utils.get_file(\"train.csv\", \"https://storage.googleapis.com/tf-datasets/titanic/train.csv\")\n",
    "titanic_lines = tf.data.TextLineDataset(titanic_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_batch_sizes(ds):\n",
    "  batch_sizes = [batch.shape[0] for batch in ds]\n",
    "  plt.bar(range(len(batch_sizes)), batch_sizes)\n",
    "  plt.xlabel('Batch number')\n",
    "  plt.ylabel('Batch size')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "titanic_batches = titanic_lines.repeat(2).batch(128)\n",
    "plot_batch_sizes(titanic_batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    | Note:\n",
    "If you need clear epoch separation, put Dataset.batch before the repeat:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "titanic_batches = titanic_lines.batch(128).repeat(3)\n",
    "\n",
    "plot_batch_sizes(titanic_batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    | Note:\n",
    "\n",
    "If you would like to perform a custom computation (for example, to collect statistics) at the end of each epoch then it's simplest to restart the dataset iteration on each epoch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128,)\n",
      "(128,)\n",
      "(128,)\n",
      "(128,)\n",
      "(116,)\n",
      "End of epoch:  0\n",
      "(128,)\n",
      "(128,)\n",
      "(128,)\n",
      "(128,)\n",
      "(116,)\n",
      "End of epoch:  1\n",
      "(128,)\n",
      "(128,)\n",
      "(128,)\n",
      "(128,)\n",
      "(116,)\n",
      "End of epoch:  2\n"
     ]
    }
   ],
   "source": [
    "epochs = 3\n",
    "dataset = titanic_lines.batch(128)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "  for batch in dataset:\n",
    "    print(batch.shape)\n",
    "  print(\"End of epoch: \", epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    | Randomly shuffling input data\n",
    "    \n",
    "The Dataset.shuffle() transformation maintains a fixed-size buffer and chooses the next element uniformly at random from that buffer.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time series windowing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<RangeDataset element_spec=TensorSpec(shape=(), dtype=tf.int64, name=None)>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "range_ds = tf.data.Dataset.range(100000)\n",
    "range_ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n",
      "[15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "[30 31 32 33 34 35 36 37 38 39 40 41 42 43 44]\n",
      "[45 46 47 48 49 50 51 52 53 54 55 56 57 58 59]\n",
      "[60 61 62 63 64 65 66 67 68 69 70 71 72 73 74]\n",
      "[75 76 77 78 79 80 81 82 83 84 85 86 87 88 89]\n",
      "[ 90  91  92  93  94  95  96  97  98  99 100 101 102 103 104]\n",
      "[105 106 107 108 109 110 111 112 113 114 115 116 117 118 119]\n",
      "[120 121 122 123 124 125 126 127 128 129 130 131 132 133 134]\n",
      "[135 136 137 138 139 140 141 142 143 144 145 146 147 148 149]\n"
     ]
    }
   ],
   "source": [
    "# Using batch\n",
    "batches = range_ds.batch(15, drop_remainder=True)\n",
    "\n",
    "for batch in batches.take(10):\n",
    "  print(batch.numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
